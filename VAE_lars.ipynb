{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3187d00a",
   "metadata": {},
   "source": [
    "# VAE using Lars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5492419e",
   "metadata": {},
   "source": [
    "This code is provided only to show our implementation of the algorithm described in the article \"Resampled Priors for Variational Autoencoders\". To evaluate our work, please look at the other .ipynb script that loads and infere our most performant model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "016815c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.01249535315117</th>\n",
       "      <th>0.0111256706670408</th>\n",
       "      <th>0.0032520459252687</th>\n",
       "      <th>0.0066249108779032</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.011439</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.006947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.007277</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.017828</td>\n",
       "      <td>0.028210</td>\n",
       "      <td>0.007758</td>\n",
       "      <td>0.007382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.021115</td>\n",
       "      <td>0.019642</td>\n",
       "      <td>0.009238</td>\n",
       "      <td>0.011499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.004966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  0.01249535315117  0.0111256706670408  0.0032520459252687   \n",
       "0  1          0.011439            0.002691            0.001206  \\\n",
       "1  2          0.000632            0.007277            0.004049   \n",
       "2  3          0.017828            0.028210            0.007758   \n",
       "3  4          0.021115            0.019642            0.009238   \n",
       "4  5          0.001177            0.002096            0.001348   \n",
       "\n",
       "   0.0066249108779032  \n",
       "0            0.006947  \n",
       "1            0.000074  \n",
       "2            0.007382  \n",
       "3            0.011499  \n",
       "4            0.004966  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = './data_train_log_return.csv'\n",
    "data_train_log_return = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0b5ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data_train_log_return.csv'\n",
    "data = pd.read_csv(file_path, header=None)\n",
    "data = data.drop(data.columns[0], axis=1)\n",
    "\n",
    "ori_data = pd.DataFrame.to_numpy(data)\n",
    "\n",
    "num_samples = 746\n",
    "data_dim = 4\n",
    "sigma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09de9345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "\n",
    "original_dim = ori_data.shape[1]   # minus 1 to exclude the index column\n",
    "latent_dim = 2  # Dimension of the latent space\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, original_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.original_dim = original_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(original_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, latent_dim * 2)  # Outputting both mean and log-variance\n",
    "        )\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, original_dim)\n",
    "        )\n",
    "\n",
    "        # Lars layers\n",
    "        self.lars = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # Outputting the probability of acceptance\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder forward pass\n",
    "        enc_output = self.encoder(x)\n",
    "        z_mean, z_log_var = torch.split(enc_output, self.latent_dim, dim=-1)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "\n",
    "        # Decoder forward pass\n",
    "        decoded = self.decoder(z)\n",
    "\n",
    "        return decoded, z, z_mean, torch.abs(z_log_var) + 0.01\n",
    "\n",
    "    def lars_forward(self, z):\n",
    "        return self.lars(z)\n",
    "\n",
    "# Instantiate the VAE model\n",
    "vae = VAE(original_dim, latent_dim)\n",
    "\n",
    "def simulate_norm_Z(model, encoder_input, z_r, z_s, Z_moving_average, eps=0.1):\n",
    "    S = z_s.shape[0]\n",
    "    R = z_r.shape[0]\n",
    "    Z_s = torch.sum(model.lars_forward(z_s)) / S\n",
    "    Z_r_curr = torch.zeros((R))\n",
    "    Z_r_smooth = torch.zeros((R))\n",
    "    Z_r = torch.zeros((R))\n",
    "    \n",
    "\n",
    "    for i in range(R):\n",
    "        x_r = encoder_input[i]\n",
    "        pi = torch.distributions.MultivariateNormal(torch.tensor([0., 0.]), torch.diag(torch.tensor([1., 1.])))\n",
    "        with torch.no_grad():\n",
    "            q = torch.distributions.MultivariateNormal(model.forward(x_r)[2], torch.diag(model.forward(x_r)[3]))\n",
    "        Z_r_curr[i] = (S * Z_s + torch.autograd.Variable(pi.log_prob(z_r[i]) / q.log_prob(z_r[i]) * model.lars_forward(z_r[i]))) / (S + 1)\n",
    "        Z_r_smooth[i] = eps * Z_r_curr[i] + (1 - eps) * Z_moving_average\n",
    "        Z_r[i] = Z_r_curr[i] + torch.autograd.Variable(Z_r_smooth[i] - Z_r_curr[i])\n",
    "\n",
    "    Z_new_moving_average = torch.sum(Z_r_smooth.detach()) / R\n",
    "    log_Z = torch.sum(torch.log(Z_r)) / R\n",
    "\n",
    "    return Z_new_moving_average, log_Z\n",
    "\n",
    "# Loss function\n",
    "def loss_function(recon_x, x, z_mean, z_log_var, a_r, log_Z):\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum') / (original_dim)\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp() - a_r) - log_Z\n",
    "    return recon_loss + kl_loss \n",
    "\n",
    "optimizer = torch.optim.Adam(vae.parameters())\n",
    "\n",
    "# Training loop example\n",
    "def train_model(model, data, epochs=50, batch_size=32):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    data = torch.Tensor(data)\n",
    "    dataset = torch.utils.data.TensorDataset(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    Z_moving_average = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            x_r = batch[0]\n",
    "            z_r = model.forward(x_r)[1]\n",
    "            z_sample = np.random.normal(size=(x_r.shape[0], latent_dim))\n",
    "            z_s = torch.Tensor(z_sample)\n",
    "            Z_new_moving_average, log_Z = simulate_norm_Z(model, x_r, z_r, z_s, Z_moving_average, eps=0.1)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, z, z_mean, z_log_var = model(batch[0])\n",
    "            a_r = model.lars_forward(z_r)\n",
    "            loss = loss_function(recon_batch, batch[0], z_mean, z_log_var, a_r, log_Z)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(data)}\")\n",
    "\n",
    "    return log_Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "711ae288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: nan\n",
      "Epoch 2/5, Loss: 0.2542198743002025\n",
      "Epoch 3/5, Loss: 0.0643997028190393\n",
      "Epoch 4/5, Loss: 0.015025316670177449\n",
      "Epoch 5/5, Loss: 0.007481997458129402\n"
     ]
    }
   ],
   "source": [
    "log_Z = train_model(vae, ori_data, epochs=5, batch_size=32)\n",
    "Z = torch.exp(log_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac2938e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33606\\AppData\\Local\\Temp\\ipykernel_24868\\4112867678.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_sample_tensor = torch.tensor(samples_bis_tensor, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "pi = torch.distributions.MultivariateNormal(torch.tensor([0., 0.]), torch.diag(torch.tensor([1., 1.])))\n",
    "\n",
    "while len(samples)<2:\n",
    "    z_sample = np.random.normal(size=(1, latent_dim))\n",
    "    acceptance_prob = vae.lars_forward(torch.tensor(z_sample, dtype=torch.float32)) / (torch.exp(pi.log_prob(torch.tensor(z_sample, dtype=torch.float32))) + 1e-10)\n",
    "    uniform_sample = torch.rand(1)  # Sample from a uniform distribution\n",
    "    if uniform_sample < acceptance_prob:\n",
    "        samples.append(z_sample)\n",
    "\n",
    "new_samples = np.array(samples)\n",
    "samples_bis = new_samples\n",
    "samples_bis_tensor = torch.tensor(samples_bis)\n",
    "z_sample_tensor = torch.tensor(samples_bis_tensor, dtype=torch.float)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
